{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from peft import PeftModel,PeftConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from prm_interface import PRM, StepScore\n",
    "from torch.types import Device\n",
    "from transformers import (  # type: ignore  # type: ignore\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "class math_psa_prm(PRM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        aggregation: str = \"full\",#the way how prm step scores will be aggregated in a solution\n",
    "        quantization_config: Optional[BitsAndBytesConfig] = None,\n",
    "        device: Optional[Device] = None,\n",
    "    ) -> None:\n",
    "        self.device = (\n",
    "            device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "\n",
    "        self.good_token = '+'\n",
    "        self.bad_token = '-'\n",
    "        self.step_tag = '\\n\\n\\n\\n\\n' #ки\n",
    "        self.step_tag2 = '\\n\\n'\n",
    "\n",
    "        self.model_path = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path,add_eos_token=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch.bfloat16).to(self.device)\n",
    "        \n",
    "        adapter_path = \"openreasoner/Math-psa\"\n",
    "\n",
    "        adapter_config = PeftConfig.from_pretrained(adapter_path)\n",
    "\n",
    "        \n",
    "        if not quantization_config:\n",
    "            self.model.to(self.device)\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "\n",
    "        self.tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "        self.tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "        self.candidate_tokens = self.tokenizer.encode(f\" {self.good_token} {self.bad_token}\") # [488, 481]\n",
    "        self.step_tag_id = self.tokenizer.encode(f\" {self.step_tag}\")[-1] # 76325\n",
    "\n",
    "\n",
    "\n",
    "    def __call_single(self, single_beam: str) -> float | list[float]:\n",
    "        input_for_prm = single_beam\n",
    "        # input_id = torch.tensor([self.tokenizer.encode(input_for_prm)]).to(self.device)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     logits = self.model(input_id).logits[:,:,self.candidate_tokens]\n",
    "        #     scores = logits.softmax(dim=-1)[:,:,0]#for the good token\n",
    "        #     step_scores = scores[input_id == self.step_tag_id]\n",
    "       \n",
    "        # step_probs  = step_scores.tolist()\n",
    "\n",
    "\n",
    "        ###\n",
    "        input_id = torch.tensor([self.tokenizer.encode(input_for_prm)]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_id).logits[:,:,self.candidate_tokens]\n",
    "            # print(logits)\n",
    "            scores = logits.softmax(dim=-1)[:,:,0] \n",
    "            # print(scores)\n",
    "            step_scores = scores[input_id == self.step_tag_id]\n",
    "            step_probs  = step_scores.tolist()\n",
    "        ###\n",
    "\n",
    "        if self.aggregation == \"min\":\n",
    "            return min(step_probs)\n",
    "        elif self.aggregation == \"max\":\n",
    "            return max(step_probs)\n",
    "        elif self.aggregation == \"mean\":\n",
    "            return statistics.mean(step_probs)\n",
    "        elif self.aggregation == \"prod\":\n",
    "            return math.prod(step_probs)\n",
    "        elif self.aggregation == \"last\":\n",
    "            return step_probs[-1]\n",
    "        elif self.aggregation == \"full\":\n",
    "            return step_probs\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __call__(self, steps: list[str]) -> list[StepScore]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            steps (list[str]): A list of reasoning solutions.\n",
    "\n",
    "        Returns:\n",
    "            list[StepScore]: A list of dictionaries where each dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for beam in steps:#each beam is a cot solution, each step_score is a list of prm step scores for this solution(if aggregation methods is full)\n",
    "            step_score = self.__call_single(beam)\n",
    "            result.append(StepScore(step=beam, score=step_score))\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [24:24<00:00, 366.12s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.63it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 44.31 GiB of which 15.69 MiB is free. Including non-PyTorch memory, this process has 1.88 GiB memory in use. Process 18716 has 40.51 GiB memory in use. Process 20646 has 1.88 GiB memory in use. Of the allocated memory 1.45 GiB is allocated by PyTorch, and 17.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prm \u001b[38;5;241m=\u001b[39m \u001b[43mmath_psa_prm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mmath_psa_prm.__init__\u001b[0;34m(self, aggregation, quantization_config, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-Math-7B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path,add_eos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m adapter_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenreasoner/Math-psa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m adapter_config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(adapter_path)\n",
      "File \u001b[0;32m/opt/conda/envs/openo1/lib/python3.12/site-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/openo1/lib/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/openo1/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/openo1/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/openo1/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/openo1/lib/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/openo1/lib/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 44.31 GiB of which 15.69 MiB is free. Including non-PyTorch memory, this process has 1.88 GiB memory in use. Process 18716 has 40.51 GiB memory in use. Process 20646 has 1.88 GiB memory in use. Of the allocated memory 1.45 GiB is allocated by PyTorch, and 17.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "prm = math_psa_prm(\n",
    "            aggregation=\"full\", \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"\"###mixedomain format json file\n",
    "data = read_json_file(json_file_path)\n",
    "\n",
    "#organizing input cot data format based on different prm data formats\n",
    "for each_data in tqdm(data):\n",
    "    for cot in each_data[\"chain_of_thoughts\"]:\n",
    "        steps = cot[\"steps\"]\n",
    "        steps = [step.replace(prm.step_tag, \"\") for step in steps]\n",
    "        updated_steps = []\n",
    "        for index, step in enumerate(steps):\n",
    "            indexed_step = f\"\\nStep {str(index+1)}: {step} {prm.step_tag}\"\n",
    "            updated_steps.append(indexed_step)\n",
    "        steps = updated_steps\n",
    "        question = each_data[\"question\"].replace(prm.step_tag, \"\")\n",
    "        steps_all = f\"{question} \" + \"\".join(steps)\n",
    "        rewards = prm([steps_all])\n",
    "        cot[\"prm_reward\"] = rewards[0].score\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from prm_interface import PRM, StepScore\n",
    "from torch.types import Device\n",
    "from transformers import (  # type: ignore  # type: ignore\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "class Mistral7bPRM(PRM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        aggregation: str = \"full\",#the way how prm step scores will be aggregated in a solution\n",
    "        quantization_config: Optional[BitsAndBytesConfig] = None,\n",
    "        device: Optional[Device] = None,\n",
    "    ) -> None:\n",
    "        self.device = (\n",
    "            device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.good_token = '+' #token in the vocabulary set that indicates the probability of good step \n",
    "        self.bad_token = '-' #token in the vocabulary set that indicates the probability of bad step \n",
    "        self.step_tag = 'ки'# step deliminator to locate the logits to get the prm score for each step\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('peiyi9979/math-shepherd-mistral-7b-prm')\n",
    "        print(type(self.tokenizer))\n",
    "        print(tokenizer)\n",
    "        \n",
    "        \n",
    "        self.candidate_tokens = self.tokenizer.encode(f\"{self.good_token} {self.bad_token}\")[1:] # [648, 387]\n",
    "        self.step_tag_id = self.tokenizer.encode(f\"{self.step_tag}\")[-1] # 12902\n",
    "        self.model = AutoModelForCausalLM.from_pretrained('peiyi9979/math-shepherd-mistral-7b-prm',\n",
    "                                                       quantization_config=quantization_config).eval()\n",
    "        if not quantization_config:\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def __call_single(self, single_beam: str) -> float | list[float]:\n",
    "        input_for_prm = single_beam\n",
    "        input_id = torch.tensor([self.tokenizer.encode(input_for_prm)]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_id).logits[:,:,self.candidate_tokens]\n",
    "            scores = logits.softmax(dim=-1)[:,:,0]#for the good token\n",
    "            step_scores = scores[input_id == self.step_tag_id]\n",
    "       \n",
    "        step_probs  = step_scores.tolist()\n",
    "\n",
    "        if self.aggregation == \"min\":\n",
    "            return min(step_probs)\n",
    "        elif self.aggregation == \"max\":\n",
    "            return max(step_probs)\n",
    "        elif self.aggregation == \"mean\":\n",
    "            return statistics.mean(step_probs)\n",
    "        elif self.aggregation == \"prod\":\n",
    "            return math.prod(step_probs)\n",
    "        elif self.aggregation == \"last\":\n",
    "            return step_probs[-1]\n",
    "        elif self.aggregation == \"full\":\n",
    "            return step_probs\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __call__(self, steps: list[str]) -> list[StepScore]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            steps (list[str]): A list of reasoning solutions.\n",
    "\n",
    "        Returns:\n",
    "            list[StepScore]: A list of dictionaries where each dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for beam in steps:#each beam is a cot solution, each step_score is a list of prm step scores for this solution(if aggregation methods is full)\n",
    "            step_score = self.__call_single(beam)\n",
    "            result.append(StepScore(step=beam, score=step_score))\n",
    "\n",
    "        return result\n",
    "    \n",
    "prm = Mistral7bPRM(\n",
    "                aggregation=\"full\", \n",
    "            )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from prm_interface import PRM, StepScore\n",
    "from torch.types import Device\n",
    "from transformers import (  # type: ignore  # type: ignore\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "class Mistral7bPRM(PRM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        aggregation: str = \"full\",#the way how prm step scores will be aggregated in a solution\n",
    "        quantization_config: Optional[BitsAndBytesConfig] = None,\n",
    "        device: Optional[Device] = None,\n",
    "    ) -> None:\n",
    "        self.device = (\n",
    "            device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.good_token = '+' #token in the vocabulary set that indicates the probability of good step \n",
    "        self.bad_token = '-' #token in the vocabulary set that indicates the probability of bad step \n",
    "        self.step_tag = 'ки'# step deliminator to locate the logits to get the prm score for each step\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('peiyi9979/math-shepherd-mistral-7b-prm')\n",
    "        # print(self.tokenizer)\n",
    "        print(type(self.tokenizer))\n",
    "        \n",
    "        \n",
    "        self.candidate_tokens = self.tokenizer.encode(f\"{self.good_token} {self.bad_token}\")[1:] # [648, 387]\n",
    "        self.step_tag_id = self.tokenizer.encode(f\"{self.step_tag}\")[-1] # 12902\n",
    "        self.model = AutoModelForCausalLM.from_pretrained('peiyi9979/math-shepherd-mistral-7b-prm',\n",
    "                                                       quantization_config=quantization_config).eval()\n",
    "        if not quantization_config:\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def __call_single(self, single_beam: str) -> float | list[float]:\n",
    "        input_for_prm = single_beam\n",
    "        input_id = torch.tensor([self.tokenizer.encode(input_for_prm)]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_id).logits[:,:,self.candidate_tokens]\n",
    "            scores = logits.softmax(dim=-1)[:,:,0]#for the good token\n",
    "            step_scores = scores[input_id == self.step_tag_id]\n",
    "       \n",
    "        step_probs  = step_scores.tolist()\n",
    "\n",
    "        if self.aggregation == \"min\":\n",
    "            return min(step_probs)\n",
    "        elif self.aggregation == \"max\":\n",
    "            return max(step_probs)\n",
    "        elif self.aggregation == \"mean\":\n",
    "            return statistics.mean(step_probs)\n",
    "        elif self.aggregation == \"prod\":\n",
    "            return math.prod(step_probs)\n",
    "        elif self.aggregation == \"last\":\n",
    "            return step_probs[-1]\n",
    "        elif self.aggregation == \"full\":\n",
    "            return step_probs\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __call__(self, steps: list[str]) -> list[StepScore]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            steps (list[str]): A list of reasoning solutions.\n",
    "\n",
    "        Returns:\n",
    "            list[StepScore]: A list of dictionaries where each dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for beam in steps:#each beam is a cot solution, each step_score is a list of prm step scores for this solution(if aggregation methods is full)\n",
    "            step_score = self.__call_single(beam)\n",
    "            result.append(StepScore(step=beam, score=step_score))\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "prm = Mistral7bPRM(\n",
    "            aggregation=\"full\", \n",
    "        )\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openo1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
